name: test-run
target: modules.train_general_llm.setup

use_neftune: true
neft_alpha: 50
use_compile: false
model_params:
  trust_remote_code: true
  attn_implementation: sdpa

use_lora: false
q_lora: false
lora_params:
  r: 64
  lora_alpha: 16
  target_modules: ["q_proj","k_proj","v_proj","o_proj","up_proj","gate_proj","down_proj"]
  lora_dropout: 0.05
  bias: "none"
  task_type: "CAUSAL_LM"

trainer:
  # model_path: mistralai/Mixtral-8x7B-v0.1
  model_path: Qwen/Qwen1.5-7B-Chat
  batch_size: 16
  seed: 1138
  wandb_id: "qwen"
  accumulate_grad_batches: 1
  gradient_clip_val: 1.0

  save_format: diffusers
  checkpoint_dir: checkpoint
  checkpoint_freq: 1
  checkpoint_steps: 10000
  eval_samples: 1000
  eval_steps: 1000
  eval_epoch: 1
  max_epochs: 60
  max_steps: -1
  
lightning:
  accelerator: gpu
  devices: -1
  precision: bf16-true 

dataset:
  name: data.text_dataset.ChatMLDataset
  max_seq_length: 1024
  cache_ids: true
  train_dataset: 
    path: hypervariance/function-calling-sharegpt
    split: 'train[:90%]'
  val_dataset: 
    path: hypervariance/function-calling-sharegpt
    split: 'train[90%:]'
  
optimizer:
  name: bitsandbytes.optim.AdamW8bit
  params:
    lr: 4e-5
    weight_decay: 1e-2

scheduler:
  name: transformers.get_constant_schedule_with_warmup
  params:
    num_warmup_steps: 50
    last_epoch: -1

sampling:
  enabled: false
  max_length: 200
  every_n_steps: 50
  every_n_epochs: 1
  prompts:
    - "what is the result of 1+1"
    - "what is the result of 2*591"
    - "what is the result of 3^2"
    - "what is the result of sqrt(128)"
