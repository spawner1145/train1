name: test-run
target: modules.train_llava.setup

model_params:
  trust_remote_code: true
  attn_implementation: sdpa

# LLaVA v1.5 pretrain config
# model_config:
#   version: plain # v1
#   vision_tower: openai/clip-vit-large-patch14-336
#   mm_projector_type: mlp2x_gelu
#   mm_vision_select_layer: -2
#   mm_use_im_start_end: false
#   mm_use_im_patch_token: false
#   pretrain_mm_mlp_adapter: null
#   mm_patch_merge_type: 'flat'
#   mm_vision_select_feature: 'patch'
#   image_aspect_ratio: 'square'
#   mm_projector_lr: 2e-5
#   tune_mm_mlp_adapter: true
#   tune_mm_vision_tower: false
#   freeze_backbone: true

# LLaVA v1.5 finetune config
# https://huggingface.co/liuhaotian/llava-v1.5-mlp2x-336px-pretrain-vicuna-7b-v1.5/resolve/main/mm_projector.bin
model_config:
  version: v1
  vision_tower: openai/clip-vit-large-patch14-336
  mm_projector_type: mlp2x_gelu
  mm_vision_select_layer: -2
  mm_use_im_start_end: false
  mm_use_im_patch_token: false
  pretrain_mm_mlp_adapter: mm_projector.bin
  mm_patch_merge_type: 'flat'
  mm_vision_select_feature: 'patch'
  image_aspect_ratio: 'pad'
  mm_projector_lr: 2e-5
  tune_mm_mlp_adapter: false
  tune_mm_vision_tower: true
  freeze_backbone: true

use_lora: true
q_lora: false
lora_params:
  r: 128
  lora_alpha: 256
  target_modules: ... # will inherit from the model_params
  lora_dropout: 0.05
  bias: "none"
  task_type: "CAUSAL_LM"

trainer:
  model_path: liuhaotian/llava-v1.5-7b
  batch_size: 32
  seed: 1138
  wandb_id: "qwen"
  accumulate_grad_batches: 1
  gradient_clip_val: 1.0

  save_format: diffusers
  checkpoint_dir: checkpoint
  checkpoint_freq: 1
  checkpoint_steps: 5000
  eval_samples: 1000
  eval_steps: 1000
  eval_epoch: 1
  max_epochs: 60
  max_steps: -1
  
lightning:
  accelerator: gpu
  devices: -1
  precision: bf16-true

dataset:
  name: data.llava_dataset.LazySupervisedDataset
  model_max_length: 2048
  data_path: /home/ubuntu/llava/blip_laion_cc_sbu_558k.json
  image_folder: /home/ubuntu/llava/
  
optimizer:
  name: bitsandbytes.optim.AdamW8bit
  params:
    lr: 4e-5
    weight_decay: 1e-2

scheduler:
  name: transformers.get_constant_schedule_with_warmup
  params:
    num_warmup_steps: 50
    last_epoch: -1

sampling:
  enabled: false
  max_length: 200
  every_n_steps: 50
  every_n_epochs: 1
  prompts:
    - "what is the result of 1+1"
    - "what is the result of 2*591"
    - "what is the result of 3^2"
    - "what is the result of sqrt(128)"
