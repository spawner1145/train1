{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "# Hide welcome message from bitsandbytes\n",
    "os.environ.update({\"BITSANDBYTES_NOWELCOME\": \"1\"})\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from diffusers import AutoencoderKL\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(\"stabilityai/sdxl-vae\")\n",
    "\n",
    "# from lib.sgm import AutoencoderKL\n",
    "# from lib.utils import load_torch_file, load_model_weights\n",
    "# from pathlib import Path    \n",
    "# from omegaconf import OmegaConf\n",
    "\n",
    "# yaml_file = Path(\"lib/model_configs/sd_xl_base.yaml\")\n",
    "# model_params = OmegaConf.load(yaml_file).model.params        \n",
    "# first_stage_model = AutoencoderKL(**model_params.first_stage_config.params)\n",
    "# state_dict = load_torch_file(\"/notebooks/ComfyUI/models/checkpoints/sd_xl_base_0.9.safetensors\")\n",
    "\n",
    "# class WeightsLoader(torch.nn.Module):\n",
    "#     pass\n",
    "        \n",
    "# w = WeightsLoader()\n",
    "# w.first_stage_model = first_stage_model\n",
    "# _ = load_model_weights(w, state_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(image_path):\n",
    "    # load image with PIL\n",
    "    img = Image.open(image_path)\n",
    "    \n",
    "    # apply transformations: resize and normalization\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(1024), # resize image\n",
    "        transforms.ToTensor(),         # convert to tensor\n",
    "        transforms.Normalize(mean=0.5, std=0.5)  # normalization\n",
    "    ])\n",
    "    \n",
    "    return transform(img).unsqueeze(0)  # create a mini-batch as expected by the model\n",
    "\n",
    "def denormalize(img, mean=0.5, std=0.5):\n",
    "    res = transforms.Normalize((-1*mean/std), (1.0/std))(img)\n",
    "    res = torch.clamp(res, 0, 1)\n",
    "    return res\n",
    "\n",
    "# Load and preprocess image\n",
    "image_path = \"/notebooks/storage/datasets/quan/009_95190271_p0_.jpg\"  # change this to your image file path\n",
    "img_tensor = load_and_preprocess_image(image_path).cuda()\n",
    "print(img_tensor.shape)\n",
    "\n",
    "# Load the model\n",
    "# vae = AutoencoderKL.from_pretrained(\"stabilityai/sdxl-vae\").cuda()\n",
    "# vae = first_stage_model\n",
    "vae.eval().cuda()\n",
    "vae.training = False\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    # img_encoded = vae.encode(img_tensor).latent_dist.sample()\n",
    "    # img_decoded = vae.decode(img_encoded).sample\n",
    "    img_encoded = vae.encode(img_tensor).sample()\n",
    "    print(img_encoded.mean(), torch.isnan(img_encoded).any())\n",
    "    img_decoded = vae.decode(img_encoded)\n",
    "\n",
    "# Convert tensors to numpy arrays for visualization\n",
    "img_tensor = denormalize(img_tensor).squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "img_decoded = denormalize(img_decoded).squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "# Display original and reconstructed images\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 5), dpi=300)\n",
    "axes[0].imshow(img_tensor)\n",
    "axes[0].set_axis_off()  # turn off the axis\n",
    "axes[0].title.set_text('Original')\n",
    "axes[1].imshow(img_decoded)\n",
    "axes[1].set_axis_off()  # turn off the axis\n",
    "axes[1].title.set_text('Reconstructed')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
